{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e2b1bce-de83-4c7a-9f60-a471229c1ca4",
   "metadata": {},
   "source": [
    "# ESG Report Sentiment Analysis: Detecting Greenwashing and Industry Priorities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a7dfbe-aaac-437d-9036-e284e54e93e8",
   "metadata": {},
   "source": [
    "Install necessary packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5befbdf1-7a68-48ed-ab53-3857b7b2b6a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.4-cp313-cp313-win_amd64.whl.metadata (3.5 kB)\n",
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: click in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: requests in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.32.3)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sonali\\anaconda3\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2025.7.14)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "   ---------------------------------------- 0.0/624.3 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 262.1/624.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 624.3/624.3 kB 3.6 MB/s eta 0:00:00\n",
      "Downloading wordcloud-1.9.4-cp313-cp313-win_amd64.whl (300 kB)\n",
      "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "Installing collected packages: PyPDF2, vaderSentiment, wordcloud, textblob\n",
      "\n",
      "   ---------------------------------------- 0/4 [PyPDF2]\n",
      "   ---------------------------------------- 0/4 [PyPDF2]\n",
      "   -------------------- ------------------- 2/4 [wordcloud]\n",
      "   ------------------------------ --------- 3/4 [textblob]\n",
      "   ---------------------------------------- 4/4 [textblob]\n",
      "\n",
      "Successfully installed PyPDF2-3.0.1 textblob-0.19.0 vaderSentiment-3.3.2 wordcloud-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2 nltk textblob pandas numpy matplotlib seaborn scikit-learn wordcloud vaderSentiment openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b4f124-3d45-4421-a6ac-cb028eb5b858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sonali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sonali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sonali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sonali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\sonali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For PDF processing\n",
    "import PyPDF2\n",
    "\n",
    "# For NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# For Sentiment Analysis\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# For TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Downloading required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fabefc9c-eab4-461c-ae6e-b8d36b105469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding keywords with synonyms:\n",
      "\n",
      "Environmental: 27 → 53 keywords\n",
      "Social: 28 → 67 keywords\n",
      "Governance: 26 → 62 keywords\n",
      "\n",
      "Final counts:\n",
      "  Environmental: 67 keywords\n",
      "  Social: 82 keywords\n",
      "  Governance: 77 keywords\n",
      "\n",
      "Sample keywords per pillar:\n",
      "\n",
      "Environmental:\n",
      "  atomic number 6, biodiversity, bionomic, carbon, carbon dioxide, carbon footprint, carbon neutral, circular economy, clean energy, climate, climate change, clime, co2, conservation, contamination...\n",
      "\n",
      "Social:\n",
      "  benefit, benefits, booking, breeding, community, community engagement, comprehension, condom, culture, customer satisfaction, development, discrimination, diverseness, diversity, diversity and inclusion...\n",
      "\n",
      "Governance:\n",
      "  accountability, administrator, answerability, anti corruption, audit, board, board independence, board of directors, brass, bribery, citizens committee, code of conduct, committee, compensation, compliance...\n",
      "\n",
      "Greenwashing indicators: 80 terms\n",
      "Substantive action words: 94 terms\n",
      "\n",
      "Keywords saved to 'expanded_esg_keywords.txt'\n"
     ]
    }
   ],
   "source": [
    "# Simple function to grab synonyms from WordNet\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace('_', ' ').lower()\n",
    "            synonyms.add(synonym)\n",
    "    return synonyms\n",
    "\n",
    "# Add synonyms to our keyword list\n",
    "def expand_keywords_with_synonyms(keywords_list, max_synonyms_per_word=3):\n",
    "    expanded = set(keywords_list)\n",
    "    \n",
    "    for keyword in keywords_list:\n",
    "        synonyms = get_synonyms(keyword)\n",
    "        for syn in list(synonyms)[:max_synonyms_per_word]:\n",
    "            if len(syn) > 2:  # skip super short words\n",
    "                expanded.add(syn)\n",
    "    \n",
    "    return sorted(list(expanded))\n",
    "\n",
    "# Starting keywords for each ESG category\n",
    "BASE_ESG_KEYWORDS = {\n",
    "    'Environmental': [\n",
    "        'climate', 'carbon', 'emission', 'renewable', 'energy', \n",
    "        'sustainability', 'environmental', 'waste', 'recycling', 'pollution',\n",
    "        'biodiversity', 'ecosystem', 'water', 'greenhouse', \n",
    "        'deforestation', 'conservation', 'footprint',\n",
    "        'sustainable', 'ecological', 'nature', 'ocean', 'forest', 'plastic',\n",
    "        'solar', 'wind', 'fossil', 'green'\n",
    "    ],\n",
    "    'Social': [\n",
    "        'employee', 'diversity', 'inclusion', 'equity', 'workforce',\n",
    "        'labor', 'community', 'safety', 'health', 'wellbeing',\n",
    "        'training', 'development', 'gender', 'equality', 'discrimination',\n",
    "        'workplace', 'welfare', 'social', 'stakeholder', 'engagement',\n",
    "        'philanthropy', 'volunteering', 'hiring', 'retention', 'culture',\n",
    "        'benefits', 'human', 'rights'\n",
    "    ],\n",
    "    'Governance': [\n",
    "        'governance', 'board', 'director', 'ethics', 'compliance',\n",
    "        'transparency', 'accountability', 'audit', 'risk', 'management',\n",
    "        'shareholder', 'executive', 'compensation', 'integrity',\n",
    "        'corruption', 'bribery', 'policy', 'regulation', 'regulatory',\n",
    "        'oversight', 'independent', 'committee', 'disclosure', 'reporting',\n",
    "        'ethical', 'leadership'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Expand keywords with synonyms\n",
    "print(\"Expanding keywords with synonyms:\\n\")\n",
    "ESG_KEYWORDS = {}\n",
    "\n",
    "for pillar, keywords in BASE_ESG_KEYWORDS.items():\n",
    "    expanded = expand_keywords_with_synonyms(keywords, max_synonyms_per_word=2)\n",
    "    ESG_KEYWORDS[pillar] = expanded\n",
    "    print(f\"{pillar}: {len(keywords)} → {len(expanded)} keywords\")\n",
    "\n",
    "# Add multi-word phrases (WordNet doesn't handle these well)\n",
    "ADDITIONAL_PHRASES = {\n",
    "    'Environmental': [\n",
    "        'renewable energy', 'clean energy', 'net zero', 'decarbonization',\n",
    "        'circular economy', 'resource efficiency', 'climate change',\n",
    "        'carbon footprint', 'carbon neutral', 'carbon dioxide', 'co2',\n",
    "        'ghg emissions', 'global warming', 'environmental impact'\n",
    "    ],\n",
    "    'Social': [\n",
    "        'human rights', 'customer satisfaction', 'supply chain', 'fair trade',\n",
    "        'living wage', 'local communities', 'work life balance', 'employee engagement',\n",
    "        'diversity and inclusion', 'pay equity', 'occupational health', 'labor rights',\n",
    "        'community engagement', 'social responsibility', 'fair labor'\n",
    "    ],\n",
    "    'Governance': [\n",
    "        'corporate governance', 'board of directors', 'risk management',\n",
    "        'executive compensation', 'conflict of interest', 'code of conduct',\n",
    "        'internal controls', 'compliance program', 'board independence',\n",
    "        'shareholder rights', 'corporate ethics', 'whistleblower protection',\n",
    "        'anti corruption', 'data privacy', 'cyber security'\n",
    "    ]\n",
    "}\n",
    "\n",
    "for pillar, phrases in ADDITIONAL_PHRASES.items():\n",
    "    ESG_KEYWORDS[pillar].extend(phrases)\n",
    "    ESG_KEYWORDS[pillar] = sorted(list(set(ESG_KEYWORDS[pillar])))\n",
    "\n",
    "print(f\"\\nFinal counts:\")\n",
    "for pillar, keywords in ESG_KEYWORDS.items():\n",
    "    print(f\"  {pillar}: {len(keywords)} keywords\")\n",
    "\n",
    "# Preview some keywords\n",
    "print(f\"\\nSample keywords per pillar:\")\n",
    "for pillar, keywords in ESG_KEYWORDS.items():\n",
    "    print(f\"\\n{pillar}:\")\n",
    "    print(f\"  {', '.join(keywords[:15])}...\")\n",
    "\n",
    "# Words that signal vague commitments (possible greenwashing)\n",
    "GREENWASHING_INDICATORS = [\n",
    "    'committed', 'commitment', 'dedication', 'dedicated', 'passionate', 'leading',\n",
    "    'strive', 'striving', 'world-class', 'best-in-class', 'innovative', 'excellence',\n",
    "    'endeavor', 'endeavoring', 'working towards', 'aiming', 'planning', 'intend',\n",
    "    'aspire', 'aspiring', 'believe', 'proud', 'excited', 'promising', 'exploring',\n",
    "    'journey', 'vision', 'ambition', 'passionate about', 'hope', 'hoping', 'desire',\n",
    "    'seek', 'seeking', 'continue to', 'ongoing', 'long term', 'future'\n",
    "]\n",
    "\n",
    "GREENWASHING_INDICATORS = expand_keywords_with_synonyms(GREENWASHING_INDICATORS, max_synonyms_per_word=2)\n",
    "print(f\"\\nGreenwashing indicators: {len(GREENWASHING_INDICATORS)} terms\")\n",
    "\n",
    "# Words that signal concrete action\n",
    "SUBSTANTIVE_WORDS = [\n",
    "    'achieved', 'reduced', 'increased', 'implemented', 'completed', 'delivered',\n",
    "    'measured', 'reported', 'certified', 'audited', 'verified', 'reached',\n",
    "    'target', 'goal', 'metric', 'data', 'performance', 'result', 'outcome',\n",
    "    'baseline', 'benchmark', 'kpi', 'indicator', 'quantified', 'tracked',\n",
    "    'invested', 'spent', 'allocated', 'million', 'billion', 'percent', 'percentage',\n",
    "    'launched', 'established', 'created', 'installed', 'deployed', 'executed'\n",
    "]\n",
    "\n",
    "SUBSTANTIVE_WORDS = expand_keywords_with_synonyms(SUBSTANTIVE_WORDS, max_synonyms_per_word=2)\n",
    "print(f\"Substantive action words: {len(SUBSTANTIVE_WORDS)} terms\")\n",
    "\n",
    "# Save to file for later reference\n",
    "with open('expanded_esg_keywords.txt', 'w') as f:\n",
    "    for pillar, keywords in ESG_KEYWORDS.items():\n",
    "        f.write(f\"\\n{pillar.upper()} ({len(keywords)} keywords)\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(', '.join(keywords) + '\\n')\n",
    "    \n",
    "    f.write(f\"\\nGREENWASHING INDICATORS ({len(GREENWASHING_INDICATORS)} terms)\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(', '.join(GREENWASHING_INDICATORS) + '\\n')\n",
    "    \n",
    "    f.write(f\"\\nSUBSTANTIVE WORDS ({len(SUBSTANTIVE_WORDS)} terms)\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(', '.join(SUBSTANTIVE_WORDS) + '\\n')\n",
    "\n",
    "print(f\"\\nKeywords saved to 'expanded_esg_keywords.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8db2bf0-4ec5-4e9b-97ca-15cb696cae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from PDF file\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            \n",
    "            for page_num in range(num_pages):\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \" \"\n",
    "        \n",
    "        return text, num_pages\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {str(e)}\")\n",
    "        return \"\", 0\n",
    "\n",
    "# Clean up extracted text\n",
    "def clean_text(text):\n",
    "    # Fix spacing issues\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Keep letters, numbers, periods, commas, percent signs\n",
    "    text = re.sub(r'[^\\w\\s\\.\\,\\%]', ' ', text)\n",
    "    # Remove standalone numbers (but keep percentages)\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32cdcf57-f122-466e-9200-8688d0bbd07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentiment using VADER and TextBlob\n",
    "def analyze_sentiment(text):  \n",
    "    # VADER sentiment scores\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "    vader_scores = vader.polarity_scores(text)\n",
    "    \n",
    "    # TextBlob sentiment\n",
    "    blob = TextBlob(text)\n",
    "    textblob_polarity = blob.sentiment.polarity\n",
    "    textblob_subjectivity = blob.sentiment.subjectivity\n",
    "    \n",
    "    # Sentence-level analysis\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    if len(sentences) > 0:\n",
    "        sentence_sentiments = [TextBlob(sent).sentiment.polarity for sent in sentences]\n",
    "        positive_sentences = sum(1 for s in sentence_sentiments if s > 0.1)\n",
    "        negative_sentences = sum(1 for s in sentence_sentiments if s < -0.1)\n",
    "        neutral_sentences = len(sentences) - positive_sentences - negative_sentences\n",
    "        avg_sentiment = np.mean(sentence_sentiments)\n",
    "        sentiment_std = np.std(sentence_sentiments)\n",
    "    else:\n",
    "        sentence_sentiments = [0]\n",
    "        positive_sentences = 0\n",
    "        negative_sentences = 0\n",
    "        neutral_sentences = 0\n",
    "        avg_sentiment = 0\n",
    "        sentiment_std = 0\n",
    "    \n",
    "    return {\n",
    "        'vader_compound': vader_scores['compound'],\n",
    "        'vader_positive': vader_scores['pos'],\n",
    "        'vader_negative': vader_scores['neg'],\n",
    "        'vader_neutral': vader_scores['neu'],\n",
    "        'textblob_polarity': textblob_polarity,\n",
    "        'textblob_subjectivity': textblob_subjectivity,\n",
    "        'avg_sentence_sentiment': avg_sentiment,\n",
    "        'sentence_sentiment_std': sentiment_std,\n",
    "        'total_sentences': len(sentences),\n",
    "        'positive_sentences': positive_sentences,\n",
    "        'negative_sentences': negative_sentences,\n",
    "        'neutral_sentences': neutral_sentences,\n",
    "        'positive_sentences_pct': (positive_sentences / len(sentences) * 100) if len(sentences) > 0 else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fb00575-c09d-480b-8a0f-c71cff4ae3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect greenwashing by comparing aspirational vs concrete language\n",
    "def detect_greenwashing(text):\n",
    "   \n",
    "    text_lower = text.lower()\n",
    "    words = word_tokenize(text_lower)\n",
    "    total_words = len(words)\n",
    "    \n",
    "    if total_words == 0:\n",
    "        return {\n",
    "            'greenwashing_indicators': 0,\n",
    "            'substantive_words': 0,\n",
    "            'greenwashing_density': 0,\n",
    "            'substantive_density': 0,\n",
    "            'greenwashing_ratio': 0,\n",
    "            'risk_level': 'Unknown',\n",
    "            'risk_score': 0\n",
    "        }\n",
    "    \n",
    "    # Count vague/aspirational language\n",
    "    greenwashing_count = 0\n",
    "    for indicator in GREENWASHING_INDICATORS:\n",
    "        greenwashing_count += text_lower.count(indicator)\n",
    "    \n",
    "    # Count concrete actions/metrics\n",
    "    substantive_count = 0\n",
    "    for word in SUBSTANTIVE_WORDS:\n",
    "        substantive_count += text_lower.count(word)\n",
    "    \n",
    "    # Calculate density per 1000 words\n",
    "    greenwashing_density = (greenwashing_count / total_words) * 1000\n",
    "    substantive_density = (substantive_count / total_words) * 1000\n",
    "    \n",
    "    # Calculate ratio: high ratio = more fluff than substance\n",
    "    if substantive_count > 0:\n",
    "        greenwashing_ratio = greenwashing_count / substantive_count\n",
    "    else:\n",
    "        greenwashing_ratio = greenwashing_count if greenwashing_count > 0 else 0\n",
    "    \n",
    "    # Risk assessment\n",
    "    risk_score = (greenwashing_ratio * 0.6) + (greenwashing_density * 0.4)\n",
    "    \n",
    "    if greenwashing_ratio > 1.5 or greenwashing_density > 15:\n",
    "        risk_level = 'High'\n",
    "    elif greenwashing_ratio > 0.8 or greenwashing_density > 8:\n",
    "        risk_level = 'Medium'\n",
    "    else:\n",
    "        risk_level = 'Low'\n",
    "    \n",
    "    return {\n",
    "        'greenwashing_indicators': greenwashing_count,\n",
    "        'substantive_words': substantive_count,\n",
    "        'greenwashing_density': round(greenwashing_density, 2),\n",
    "        'substantive_density': round(substantive_density, 2),\n",
    "        'greenwashing_ratio': round(greenwashing_ratio, 2),\n",
    "        'risk_level': risk_level,\n",
    "        'risk_score': round(risk_score, 2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecc1ce16-4035-4a79-b61e-120d346df19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate which ESG pillars the company focuses on\n",
    "def calculate_esg_importance(text, company_name=\"Company\"):\n",
    "   \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Count keyword mentions for each pillar\n",
    "    esg_counts = {}\n",
    "    for pillar, keywords in ESG_KEYWORDS.items():\n",
    "        count = 0\n",
    "        for keyword in keywords:\n",
    "            count += text_lower.count(keyword.lower())\n",
    "        esg_counts[pillar] = count\n",
    "    \n",
    "    total_keywords = sum(esg_counts.values())\n",
    "    \n",
    "    if total_keywords == 0:\n",
    "        return {\n",
    "            'Environmental_count': 0,\n",
    "            'Social_count': 0,\n",
    "            'Governance_count': 0,\n",
    "            'Environmental_pct': 0,\n",
    "            'Social_pct': 0,\n",
    "            'Governance_pct': 0,\n",
    "            'dominant_pillar': 'None',\n",
    "            'total_esg_keywords': 0\n",
    "        }\n",
    "    \n",
    "    # Calculate percentages\n",
    "    env_pct = (esg_counts['Environmental'] / total_keywords) * 100\n",
    "    soc_pct = (esg_counts['Social'] / total_keywords) * 100\n",
    "    gov_pct = (esg_counts['Governance'] / total_keywords) * 100\n",
    "    \n",
    "    # Find dominant pillar\n",
    "    dominant_pillar = max(esg_counts.items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "    return {\n",
    "        'Environmental_count': esg_counts['Environmental'],\n",
    "        'Social_count': esg_counts['Social'],\n",
    "        'Governance_count': esg_counts['Governance'],\n",
    "        'Environmental_pct': round(env_pct, 2),\n",
    "        'Social_pct': round(soc_pct, 2),\n",
    "        'Governance_pct': round(gov_pct, 2),\n",
    "        'dominant_pillar': dominant_pillar,\n",
    "        'total_esg_keywords': total_keywords\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7a5c42e-50c3-4d00-91eb-7daa84310402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TF-IDF to compare ESG focus across companies\n",
    "def tfidf_esg_analysis(texts_dict):\n",
    "   \n",
    "    companies = list(texts_dict.keys())\n",
    "    documents = list(texts_dict.values())\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for pillar, keywords in ESG_KEYWORDS.items():\n",
    "        # TF-IDF with ESG keywords as vocabulary\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            vocabulary=keywords,\n",
    "            lowercase=True,\n",
    "            token_pattern=r'\\b\\w+\\b'\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "            \n",
    "            # Get scores for each company\n",
    "            for idx, company in enumerate(companies):\n",
    "                doc_scores = tfidf_matrix[idx].toarray().flatten()\n",
    "                mean_tfidf = np.mean(doc_scores) if len(doc_scores) > 0 else 0\n",
    "                max_tfidf = np.max(doc_scores) if len(doc_scores) > 0 else 0\n",
    "                \n",
    "                results.append({\n",
    "                    'company': company,\n",
    "                    'pillar': pillar,\n",
    "                    'tfidf_mean': round(mean_tfidf, 4),\n",
    "                    'tfidf_max': round(max_tfidf, 4)\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"TF-IDF issue for {pillar}: {e}\")\n",
    "            for company in companies:\n",
    "                results.append({\n",
    "                    'company': company,\n",
    "                    'pillar': pillar,\n",
    "                    'tfidf_mean': 0,\n",
    "                    'tfidf_max': 0\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5307e908-976d-4ff0-8d61-3162e61fef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_single_esg_report(pdf_path, company_name, industry, controversy_level=\"Low\"):\n",
    "    \"\"\"Run complete analysis on one ESG report\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing: {company_name}\")\n",
    "    print(f\"Industry: {industry} | Controversy: {controversy_level}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Extract text\n",
    "    text, num_pages = extract_text_from_pdf(pdf_path)\n",
    "    if not text or len(text) < 100:\n",
    "        print(\"Failed to extract text\")\n",
    "        return None\n",
    "    \n",
    "    clean_text_content = clean_text(text)\n",
    "    word_count = len(word_tokenize(clean_text_content))\n",
    "    \n",
    "    print(f\"Extracted {len(text):,} characters from {num_pages} pages\")\n",
    "    print(f\"Word count: {word_count:,}\")\n",
    "    \n",
    "    # Sentiment analysis\n",
    "    sentiment = analyze_sentiment(clean_text_content)\n",
    "    print(f\"\\nSentiment:\")\n",
    "    print(f\"  VADER: {sentiment['vader_compound']:.3f}\")\n",
    "    print(f\"  TextBlob: {sentiment['textblob_polarity']:.3f}\")\n",
    "    print(f\"  Positive sentences: {sentiment['positive_sentences_pct']:.1f}%\")\n",
    "    \n",
    "    # Greenwashing detection\n",
    "    greenwashing = detect_greenwashing(clean_text_content)\n",
    "    print(f\"\\nGreenwashing:\")\n",
    "    print(f\"  Risk: {greenwashing['risk_level']}\")\n",
    "    print(f\"  Ratio: {greenwashing['greenwashing_ratio']:.2f}\")\n",
    "    print(f\"  Aspirational words: {greenwashing['greenwashing_indicators']}\")\n",
    "    print(f\"  Substantive words: {greenwashing['substantive_words']}\")\n",
    "    \n",
    "    # ESG pillar analysis\n",
    "    esg_importance = calculate_esg_importance(clean_text_content, company_name)\n",
    "    print(f\"\\nESG Focus:\")\n",
    "    print(f\"  Environmental: {esg_importance['Environmental_pct']:.1f}%\")\n",
    "    print(f\"  Social: {esg_importance['Social_pct']:.1f}%\")\n",
    "    print(f\"  Governance: {esg_importance['Governance_pct']:.1f}%\")\n",
    "    print(f\"  Dominant: {esg_importance['dominant_pillar']}\")\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        'company_name': company_name,\n",
    "        'industry': industry,\n",
    "        'controversy_level': controversy_level,\n",
    "        'num_pages': num_pages,\n",
    "        'word_count': word_count,\n",
    "        'text_content': clean_text_content,\n",
    "    }\n",
    "    \n",
    "    results.update(sentiment)\n",
    "    results.update(greenwashing)\n",
    "    results.update(esg_importance)\n",
    "    \n",
    "    print(f\"\\nAnalysis complete for {company_name}\\n\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e802d60-6d91-481e-ae7a-75f503b46a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyze all ESG reports for one industry\n",
    "def analyze_industry_reports(pdf_folder, industry_name, company_info):\n",
    "   \n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"INDUSTRY: {industry_name}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    \n",
    "    all_results = []\n",
    "    texts_for_tfidf = {}\n",
    "    \n",
    "    for filename, (company_name, controversy) in company_info.items():\n",
    "        pdf_path = os.path.join(pdf_folder, filename)\n",
    "        \n",
    "        if not os.path.exists(pdf_path):\n",
    "            print(f\"File not found: {pdf_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Analyze report\n",
    "        result = analyze_single_esg_report(pdf_path, company_name, industry_name, controversy)\n",
    "        \n",
    "        if result:\n",
    "            all_results.append(result)\n",
    "            texts_for_tfidf[company_name] = result['text_content']\n",
    "    \n",
    "    # Create results dataframe\n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # TF-IDF analysis across companies\n",
    "    if len(texts_for_tfidf) > 1:\n",
    "        print(f\"\\nRunning TF-IDF analysis across {len(texts_for_tfidf)} companies...\")\n",
    "        tfidf_df = tfidf_esg_analysis(texts_for_tfidf)\n",
    "    else:\n",
    "        tfidf_df = pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Industry analysis complete: {industry_name}\")\n",
    "    print(f\"Companies analyzed: {len(all_results)}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return df, tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8506e5dd-dc53-4f01-b1e5-78d9412d45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze all ESG reports for one industry\n",
    "def analyze_industry_reports(pdf_folder, industry_name, company_info):\n",
    "   \n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"INDUSTRY: {industry_name}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    \n",
    "    all_results = []\n",
    "    texts_for_tfidf = {}\n",
    "    \n",
    "    for filename, (company_name, controversy) in company_info.items():\n",
    "        pdf_path = os.path.join(pdf_folder, filename)\n",
    "        \n",
    "        if not os.path.exists(pdf_path):\n",
    "            print(f\"File not found: {pdf_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Analyze report\n",
    "        result = analyze_single_esg_report(pdf_path, company_name, industry_name, controversy)\n",
    "        \n",
    "        if result:\n",
    "            all_results.append(result)\n",
    "            texts_for_tfidf[company_name] = result['text_content']\n",
    "    \n",
    "    # Create results dataframe\n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # TF-IDF analysis across companies\n",
    "    if len(texts_for_tfidf) > 1:\n",
    "        print(f\"\\nRunning TF-IDF analysis across {len(texts_for_tfidf)} companies...\")\n",
    "        tfidf_df = tfidf_esg_analysis(texts_for_tfidf)\n",
    "    else:\n",
    "        tfidf_df = pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Industry analysis complete: {industry_name}\")\n",
    "    print(f\"Companies analyzed: {len(all_results)}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return df, tfidf_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5d07e8-40fd-4799-a581-9d6e93ed0bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURE YOUR PROJECT HERE\n",
    "# ============================================\n",
    "\n",
    "# Your industry name\n",
    "INDUSTRY_NAME = \"Fashion Retail\"\n",
    "\n",
    "# Path to folder with PDF files\n",
    "PDF_FOLDER = \"./Fashion Retail\"  # Update this to your actual path\n",
    "\n",
    "# Map PDF filenames to (company_name, controversy_level)\n",
    "# controversy_level: \"Low\", \"Medium\", or \"High\"\n",
    "COMPANY_INFO = {\n",
    "    \"Aritzia.pdf\": (\"Aritzia\", \"Low\"),\n",
    "    \"FastRetailing.pdf\": (\"Fast Retailing (Uniqlo)\", \"Medium\"),\n",
    "    \"GAPInc.pdf\": (\"GAP Inc\", \"Medium\"),\n",
    "    \"H&M.pdf\": (\"H&M\", \"High\"),\n",
    "    \"Inditex.pdf\": (\"Inditex (Zara)\", \"Medium\"),\n",
    "    \"Levis.pdf\": (\"Levi's\", \"Low\"),\n",
    "    \"LMVH.pdf\": (\"LVMH\", \"Low\"),\n",
    "    \"Lululemon.pdf\": (\"Lululemon\", \"Low\"),\n",
    "    \"M&S.pdf\": (\"Marks & Spencer\", \"Low\"),\n",
    "    \"Patagonia.pdf\": (\"Patagonia\", \"Low\"),\n",
    "    \"Prada.pdf\": (\"Prada\", \"Low\"),\n",
    "    \"RalphLauren.pdf\": (\"Ralph Lauren\", \"Low\"),\n",
    "    \"Shein.pdf\": (\"Shein\", \"High\"),\n",
    "    \"TJX.pdf\": (\"TJX Companies\", \"Medium\"),\n",
    "    \"VSCo.pdf\": (\"Victoria's Secret\", \"Medium\"),\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Industry: {INDUSTRY_NAME}\")\n",
    "print(f\"  PDF folder: {PDF_FOLDER}\")\n",
    "print(f\"  Companies: {len(COMPANY_INFO)}\")\n",
    "print(f\"\\nCompanies to analyze:\")\n",
    "for filename, (company, controversy) in COMPANY_INFO.items():\n",
    "    print(f\"  - {company} ({controversy} controversy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1d03a9-f034-44de-baa5-f5cc57a741a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete analysis\n",
    "results_df, tfidf_df = analyze_industry_reports(PDF_FOLDER, INDUSTRY_NAME, COMPANY_INFO)\n",
    "\n",
    "# Show results preview\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS PREVIEW\")\n",
    "print(\"=\"*60)\n",
    "display(results_df.head())\n",
    "\n",
    "# Show key columns\n",
    "print(\"\\nKey Metrics:\")\n",
    "key_cols = ['company_name', 'controversy_level', 'vader_compound', \n",
    "            'risk_level', 'greenwashing_ratio', 'dominant_pillar']\n",
    "display(results_df[key_cols])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
